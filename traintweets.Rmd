---
title: "Analysing train company tweets"
output: html_notebook
---

# Analysing train company tweets

This notebook details the process of analysing tweets by train company accounts.

We are interested in the following questions:

* Which terms appear most frequently in the tweets?
* How does that term frequency change over time?
* Who uses those terms most/least?
* Which accounts are most active (in terms of being responded to)?

## Import the data

First we need to import the data, which is over 100,000 tweets. There are a few ways of doing this: by using the link to the latest CSV, online...

```{r, eval=FALSE, include=FALSE}
#DON'T run this code - it will take too long!
#store the location of the CSV
csvurl <- "https://premium.scraperwiki.com/3epab2y/iyjilhnvxo69dnp/cgi-bin/csv/traincompanytweets.csv"
#import the data
tweetsdata <- read.csv(csvurl, stringsAsFactors = FALSE)
```

...or by using the API endpoint which provides it in JSON:

```{r, eval=FALSE, include=FALSE}
#Don't run this code either, as it will only provide a sample to work with
#install.packages("jsonlite")
#Activate the json package - uncomment the line above to install it first if you get an error
library(jsonlite)
library(httr)
#Store the URL which queries the scraper's API for all tweets
#This is limited to 100 for testing purposes
jsonurl <- "https://premium.scraperwiki.com/3epab2y/iyjilhnvxo69dnp/sql/?q=%20select%20*%0A%20from%20traincompanytweets%20%0Alimit%20100"
tweets100 <- jsonlite::fromJSON(jsonurl)
```

You can also use this to get the results of SQL queries:

```{r}
countbyaccount <- jsonlite::fromJSON("https://premium.scraperwiki.com/3epab2y/iyjilhnvxo69dnp/sql/?q=%20select%20count(*)%2C%20account%0A%20from%20traincompanytweets%20%0A%20group%20by%20account%0A%20order%20by%20count(*)%20desc")
```


...Or by importing a local file (the disadvantage of this is that it is not a live link to the latest data):

```{r}
#Store the filename
csvfile = "traincompanytweets.csv"
tweets <- read.csv(csvfile)
```

Next some overview of accounts:

```{r}
table(tweets$account)
```

And a summary of columns:

```{r}
summary(tweets)
```

## Which terms are most common?

We are expecting that phrases like 'crew' or 'tracks' might occur particularly frequently. Let's find out which ones.

First, we need to export the column of keywords:

```{r}
#This is based on steps outlined in a [blog post by John Victor Anderson](http://johnvictoranderson.org/?p=115). 
write.csv(tweets$tweettxt, 'tweetsastext.txt')
```

Now we re-import that data as a character object using `scan`:

```{r}
tweettext <- scan('tweetsastext.txt', what="char", sep=",")
# We convert all text to lower case to prevent any case sensitive issues with counting
tweettext <- tolower(tweettext)
#Repace quotes because each tweet starts and ends with one
tweettext <- gsub('"', '', tweettext)
#Replace new line code with a space
tweettext <- gsub('\n', ' ', tweettext)
#Unescape HTML - first activate the htmltools package
library(htmltools)
#Then run the htmlEscape function
tweettext <- htmltools::htmlEscape(tweettext)
#This doesn't seem to work 100%
```

We now need to put this through a series of conversions before we can generate a table:

```{r}
#Split the text on every space
tweettext.split <- strsplit(tweettext, " ")
#Create a vector
tweettextvec <- unlist(tweettext.split)
#Convert that to a table
tweettexttable <- table(tweettextvec)
#remove the objects created that we no longer need
rm(tweettext.split, tweettextvec)
```

That table is enough to create a CSV from:

```{r}
write.csv(tweettexttable, 'tweettexttable.csv')
#read it back in
tweetdata <- read.csv('tweettexttable.csv')
summary(tweetdata)
#rename the columns
colnames(tweetdata) <- c('index', 'word', 'freq' )
summary(tweetdata)
```
### Removing stopwords

We could strip out stopwords from our data [using `tidytext`'s stop words](https://stackoverflow.com/questions/43441884/removing-stop-words-with-tidytext?rq=1).

```{r}
#Install the tidyverse package
library(tidyverse)
#Install tidytext which is needed for get_stopwords() 
#install.packages("tidytext")
#Activate that - uncomment line above if you get an error here
library(tidytext)

#Use anti_join with stopwords fetched using get_stopwords to remove those stopwords from tweetdata and put in new object
cleaned_tweetdata <- tweetdata %>%
  anti_join(get_stopwords())
```

Let's use a simpler approach to remove punctuation

```{r}
cleaned_tweetdata$wordnopunc <- gsub(",","",cleaned_tweetdata$word)
cleaned_tweetdata$wordnopunc <- gsub("-","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub("!","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub("'","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub('"',"",cleaned_tweetdata$wordnopunc)
#This has to be escaped or it replaces all characters
cleaned_tweetdata$wordnopunc <- gsub("\\.","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$wordnopunc <- gsub("\\?","",cleaned_tweetdata$wordnopunc)
cleaned_tweetdata$word2 <- NULL
```



## SQL query: most common words

We can bring the most frequent words to the top using `sqldf`:

```{r}
sqldf::sqldf('SELECT wordnopunc, freq 
             FROM cleaned_tweetdata
             ORDER BY freq DESC
             LIMIT 100')
```

## Sorry seems to be the hardest word - but it's not the only one

'Sorry' is just one type of apology. Let's add a new column that identifies variants:

```{r}
#Generate a TRUE/FALSE column which returns true if 'sorry' or 'apol' is anywhere in the text
cleaned_tweetdata$sorry <- grepl("sorry|apol", cleaned_tweetdata$word)
#Because 'sorry' is a TRUE/FALSE logical column, we need to use 1 or 0 to indicate that in sqldf
#See https://stackoverflow.com/questions/41433927/sqldf-cant-read-logical-vectors-in-r
sqldf::sqldf("SELECT * FROM cleaned_tweetdata 
             WHERE sorry = 1
             ORDER BY freq DESC")
```

What about a grand total:

```{r}
#Use sum to add up all the results and just show that
sqldf::sqldf("SELECT sum(freq), sorry FROM cleaned_tweetdata 
             GROUP BY sorry
             ORDER BY freq DESC")
```

Those totals refer to individual words rather than tweets, so we could repeat it for the tweets instead:

```{r}
#Generate a TRUE/FALSE column which returns true if 'sorry' or 'apol' is anywhere in the text
tweets$sorry <- grepl("sorry|apol", tweets$tweettxt)
#Because 'sorry' is a TRUE/FALSE logical column, we need to use 1 or 0 to indicate that in sqldf
#See https://stackoverflow.com/questions/41433927/sqldf-cant-read-logical-vectors-in-r
sorrytotals <- sqldf::sqldf("SELECT count(*), sorry FROM tweets 
             GROUP BY sorry
             ")
sorrytotals
```

So 15% of tweets:

```{r}
sorrytotals$`count(*)`[2]/sum(sorrytotals$`count(*)`)*100
```


## A word cloud?

```{r}
#Adapted from https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
#install.packages("wordcloud")
#Uncomment the line above if the line below throws an error about the package not being installed
library(wordcloud)

#Pipe the cleaned data into the count function, taking the word column
cleaned_tweetdata %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100)) #generate a word cloud
#This looks awful because it includes numbers that recur but mean nothing
```

## A breakdown by account

We can group by account to see which ones are most sorry:

```{r}
sorrybyaccount <- sqldf::sqldf("SELECT count(*), account, sorry FROM tweets
             GROUP BY account, sorry
             ")
sorrybyaccount
write_csv(sorrybyaccount, "sorrybyaccount")
```

The worst is Northern, so let's import that data, then export it as a CSV to be analysed manually in Excel if we need:

```{r}
northernjson <- "https://premium.scraperwiki.com/3epab2y/iyjilhnvxo69dnp/sql/?q=%20select%20*%0A%20from%20traincompanytweets%20%0A%20where%20account%20is%20%22northernassist%22%0A%20"
northerntweets <- jsonlite::fromJSON(northernjson)
write_csv(northerntweets,"northerntweets.csv")
```

### The worst day of the week

Or the worst day of the week

```{r}
#LEFT is called LEFTSTR in sqldf: https://stackoverflow.com/questions/31843373/how-to-use-right-left-to-split-a-variable-in-sqldf-as-in-leftx-n
sorrybyweekday <- sqldf::sqldf("SELECT count(*), LEFTSTR(tweetdate, 3) AS tweetday, sorry FROM tweets
             GROUP BY tweetday, sorry
             ")
sorrybyweekday
write_csv(sorrybyweekday, "sorrybyweekday.csv")
```

## A breakdown over time

We can use the same approach to pull out each month's apologies:

```{r}
#SUBSTR is the sqldf function to extract from the middle of text, like MID in Excel
sorrybymonth <- sqldf::sqldf("SELECT count(*), SUBSTR(tweetdate, 5,3) AS tweetday, sorry FROM tweets
             GROUP BY tweetday, sorry
             ")
sorrybymonth
write_csv(sorrybymonth, "sorrybymonth.csv")
```

There are 12 months there, so that means different accounts' tweets cover different periods:

```{r}
#SUBSTR is the sqldf function to extract from the middle of text, like MID in Excel
accountbymonth <- sqldf::sqldf("SELECT count(*), SUBSTR(tweetdate, 5,3) AS tweetday, account FROM tweets
             GROUP BY tweetday, account ORDER by account
             ")
accountbymonth
write_csv(accountbymonth, "accountbymonth.csv")
```

## Using lubridate to clean dates

Because the dates are stored as strings, we need to extract and convert parts of those strings into data that can be understood as dates by R. The `lubridate` library is [designed for this](https://r4ds.had.co.nz/data-import.html#readr-datetimes):

```{r}
#Activate the lubridate library
library(lubridate)
#Store an example so we can see it to work with
dateeg <- "Fri Oct 05 17:23:36 +0000 2018"
#Count how many characters in that, so we can use substr
nchar(dateeg)
#We could extract parts of the date as strings, based on their position...
tweets$tweetweekday <- substr(tweets$tweetdate,0,3)
tweets$tweetmonth <- substr(tweets$tweetdate,5,7)
#Repeat for the others and store them as numeric to help with using later
tweets$tweetyear <- as.numeric(substr(tweets$tweetdate,26,30))
tweets$tweetday <- as.numeric(substr(tweets$tweetdate,9,10))
tweets$tweethour <- as.numeric(substr(tweets$tweetdate,12,13))
tweets$tweetmin <- as.numeric(substr(tweets$tweetdate,15,16))
tweets$tweetsec <- as.numeric(substr(tweets$tweetdate,18,19))
#But we could also extract them as numerical values using lubridate
#Unfortunately, the format - day-month-day - doesn't fit a pre-existing function so we need to create a format and then pull that in using parse_date
tweets$fulldate <- paste(substr(tweets$tweetdate,9,10),tweets$tweetmonth,tweets$tweetyear,tweets$tweethour,substr(tweets$tweetdate,15,16),tweets$tweetsec, sep="-")
#Test 05-Oct-2018-17-23-36
parse_datetime("05-Oct-2018-17-23-36", "%d-%b-%Y-%H-%M-%S")
tweets$realdate <- parse_datetime(tweets$fulldate, "%d-%b-%Y-%H-%M-%S")
```

Now we should be able to identify the earliest and latest tweets:

```{r}
min(tweets$realdate)
max(tweets$realdate)
```

And show the earliest ones:

```{r}
attach(tweets)
head(tweets[order(realdate),])
detach(tweets)
```

These seem to be mostly from NRE_TfLRail

Let's create a filtered subset which only covers a certain period:

```{r}
tweetsnovlater <- tweets %>%
  select(tweetdate, tweettxt, tweeturl, name, screenname, account, realdate, sorry) %>%
  filter(realdate >= as.Date("2018-11-01") )
```

And check what sorts of accounts there are:

```{r}
table(tweetsnovlater$account)
```

Some have very few recent updates: ArrivaTW has 0, Elizabethline only 6, and HeathrowExpress just 350 for example.

Let's see what there is pre-Nov:

```{r}
tweetsprenov <- tweets %>%
  select(tweetdate, tweettxt, tweeturl, name, screenname, account, realdate, sorry) %>%
  filter(realdate < as.Date("2018-11-01") )
```

And again the accounts:

```{r}
table(tweetsprenov$account)
```

Here ArrivaTW has 3232 - all of their tweets - Elizabethline most of theirs, and HeathrowExpress 2869. In contrast, northernassist and LNER have 0 - all of their tweets are in November or later.

Note that a `sqldf` query will return the datetime as a number:

```{r}
sqldf::sqldf("SELECT MIN(realdate) as earliest FROM tweets WHERE account = 'northernassist'")
```

We know that northernassist tweets start on Nov 20 so we could take that as a starting date:

```{r}
tweetsnov20on <- tweets %>%
  select(tweetdate, tweettxt, tweeturl, name, screenname, account, realdate, sorry) %>%
  filter(realdate >= as.Date("2018-11-20") )
table(tweetsnov20on$account)
```

This leaves out operators like ArrivaTW - replaced by @tfwrail in October.

## Re-running the word count code

Now we can re-run the word count code on the new filtered data:

```{r}
#This is based on steps outlined in a [blog post by John Victor Anderson](http://johnvictoranderson.org/?p=115). 
write.csv(tweetsnov20on$tweettxt, 'tweetsastext.txt')
#Now we re-import that data as a character object using scan:
tweettext <- scan('tweetsastext.txt', what="char", sep=",")
# We convert all text to lower case to prevent any case sensitive issues with counting
tweettext <- tolower(tweettext)
#Repace quotes because each tweet starts and ends with one
tweettext <- gsub('"', '', tweettext)
#Replace new line code with a space
tweettext <- gsub('\n', ' ', tweettext)
#Unescape HTML - first activate the htmltools package
#library(htmltools)
#Then run the htmlEscape function
tweettext <- htmltools::htmlEscape(tweettext)
#This doesn't seem to work 100%
#We now need to put this through a series of conversions before we can generate a table:
#Split the text on every space
tweettext.split <- strsplit(tweettext, " ")
#Create a vector
tweettextvec <- unlist(tweettext.split)
#Convert that to a table
tweettexttable <- table(tweettextvec)
#remove the objects created that we no longer need
rm(tweettext.split, tweettextvec)
#That table is enough to create a CSV from:
write.csv(tweettexttable, 'tweettexttable.csv')
#read it back in
tweetdata.nov20 <- read.csv('tweettexttable.csv')
summary(tweetdata.nov20)
#rename the columns
colnames(tweetdata.nov20) <- c('index', 'word', 'freq' )
summary(tweetdata.nov20)

#Install the tidyverse package
#library(tidyverse)
#Install tidytext which is needed for get_stopwords() 
#install.packages("tidytext")
#Activate that - uncomment line above if you get an error here
#library(tidytext)

#Use anti_join with stopwords fetched using get_stopwords to remove those stopwords from tweetdata and put in new object
cleaned_tweetdata.nov20 <- tweetdata.nov20 %>%
  anti_join(get_stopwords())

cleaned_tweetdata.nov20$wordnopunc <- gsub(",","",cleaned_tweetdata.nov20$word)
cleaned_tweetdata.nov20$wordnopunc <- gsub("-","",cleaned_tweetdata.nov20$wordnopunc)
cleaned_tweetdata.nov20$wordnopunc <- gsub("!","",cleaned_tweetdata.nov20$wordnopunc)
cleaned_tweetdata.nov20$wordnopunc <- gsub("'","",cleaned_tweetdata.nov20$wordnopunc)
cleaned_tweetdata.nov20$wordnopunc <- gsub('"',"",cleaned_tweetdata.nov20$wordnopunc)
#This has to be escaped or it replaces all characters
cleaned_tweetdata.nov20$wordnopunc <- gsub("\\.","",cleaned_tweetdata.nov20$wordnopunc)
cleaned_tweetdata.nov20$wordnopunc <- gsub("\\?","",cleaned_tweetdata.nov20$wordnopunc)
```


```{r}
#Generate a TRUE/FALSE column which returns true if 'sorry' or 'apol' is anywhere in the text
tweetsnov20on$sorry <- grepl("sorry|apol", tweetsnov20on$tweettxt)
#Because 'sorry' is a TRUE/FALSE logical column, we need to use 1 or 0 to indicate that in sqldf
#See https://stackoverflow.com/questions/41433927/sqldf-cant-read-logical-vectors-in-r
sorrytotals.nov20 <- sqldf::sqldf("SELECT count(*), sorry FROM tweetsnov20on 
             GROUP BY sorry
             ")
sorrytotals.nov20
```

## Looking for connections: network analysis

```{r}
#Test the query
sqldf::sqldf('SELECT screenname, account, count(*) 
             FROM tweets WHERE account != ""
GROUP BY account, screenname
ORDER BY count(*) DESC
             LIMIT 100')
#store the results
networkanalysis <- sqldf::sqldf('SELECT screenname, account, count(*) 
             FROM tweets WHERE account != ""
GROUP BY account, screenname
ORDER BY count(*) DESC
             LIMIT 100')
#Export
write.csv(networkanalysis, "tweetnetwork.csv")
```

### Filter out Elizabethline and NRE_TfLRail - TBC

Elizabethline and NRE_TfLRail appear too few times


## Looking for emojis - TBC

The string -\n\n\ud83c\udfab appears in some tweets - this is actually JavaScript/JSON for [the ticket emoji](https://emojiterra.com/ticket/): `\ud83c\udfab`

We can use this sort of pattern with regex to identify emoji-like codes:

```{r}
#This is some great regex from https://thekevinscott.com/emojis-in-javascript/
emojiregex <- "(?:[\u2700-\u27bf]|(?:\ud83c[\udde6-\uddff]){2}|[\ud800-\udbff][\udc00-\udfff]|[\u0023-\u0039]\ufe0f?\u20e3|\u3299|\u3297|\u303d|\u3030|\u24c2|\ud83c[\udd70-\udd71]|\ud83c[\udd7e-\udd7f]|\ud83c\udd8e|\ud83c[\udd91-\udd9a]|\ud83c[\udde6-\uddff]|[\ud83c\ude01-\ude02]|\ud83c\ude1a|\ud83c\ude2f|[\ud83c\ude32-\ude3a]|[\ud83c\ude50-\ude51]|\u203c|\u2049|[\u25aa-\u25ab]|\u25b6|\u25c0|[\u25fb-\u25fe]|\u00a9|\u00ae|\u2122|\u2139|\ud83c\udc04|[\u2600-\u26FF]|\u2b05|\u2b06|\u2b07|\u2b1b|\u2b1c|\u2b50|\u2b55|\u231a|\u231b|\u2328|\u23cf|[\u23e9-\u23f3]|[\u23f8-\u23fa]|\ud83c\udccf|\u2934|\u2935|[\u2190-\u21ff])"
#...But it doesn't work here
#This is a literal code for the 'OK' hand sign
emojiregex <- "\\ud83d\\udc4c"
#Generate a TRUE/FALSE column which returns true if a code appears
cleaned_tweetdata$emoji <- str_detect(cleaned_tweetdata$word, regex(emojiregex))


```

But that doesn't quite work...
